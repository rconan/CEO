% -*- mode: Noweb; noweb-code-mode: c-mode -*-
The iterativeSolvers structure is written in the header:
<<iterativeSolvers.h>>=
#ifndef __ITERATIVESOLVERS_H__
#define __ITERATIVESOLVERS_H__

#ifndef __CEO_H__
#include "ceo.h"
#endif

#ifndef __BTBT_H__
#include "BTBT.h"
#endif

//#define MINRES_DEBUG

struct iterativeSolvers {

  <<parameters>>

  void cg_setup(int n_vector);

  void minres_setup(int n_vector);

  void cleanup(void);

  void cg(float *x, BTBT* A, float *b, int max_it, float* x0);

  void sym_ortho(float *c,float *s,float *r,
		 float *a,float *b);

  void lanczos_step(float *alpha, float *beta, float *nu_kp1,
		    BTBT *A, float *nu_k, float *nu_km1, float sigma);


  void minres_vorst(float *x, BTBT* A, float *b, int max_it, float* x0);
  void minres_vorst(float *x, float *res, BTBT* A, float *b, int max_it, float* x0);

  void minres_choi(float *x, BTBT* A, float *b, int max_it, float* x0);

  //  void info(void);
};
#endif // __ITERATIVESOLVERS_H__
@ The routines are expanded in the source:
<<iterativeSolvers.cu>>=
#include "iterativeSolvers.h"

<<CG setup>>

<<MINRES setup>>

<<cleanup>>

<<conjugate gradient>>

<<sym-ortho algorithm>>

<<Lanczos step>>

<<MINRES (Vorst)>>
<<MINRES (Vorst) with residue>>
<<MINRES (Choi)>>
@
The iterativeSolvers structure gather several routines implementing iterative methods to solve the linear system: $Ax=b$.
The setup method is allocated the required memory depending of the iterative method type:
\begin{itemize}
\item
<<CG setup>>=
void iterativeSolvers::cg_setup(int n_vector)
{
  printf("\n@(CEO)>iterativeSolvers: CONJUGATE GRADIENT\n");
  N = n_vector;
  HANDLE_ERROR( cudaMalloc((void**)&d__vectors, sizeof(float)*N*4 ) );
  q = d__vectors;
  x = d__vectors + N;
  r = d__vectors + 2*N;
  p = d__vectors + 3*N;
  cublasCreate(&handle);
}
@  \item
<<MINRES setup>>=
void iterativeSolvers::minres_setup(int n_vector)
{
  printf("\n@(CEO)>iterativeSolvers: MINRES\n");
  N = n_vector;
  HANDLE_ERROR( cudaMalloc((void**)&d__vectors, sizeof(float)*N*6 ) );
  nu_i   = d__vectors;
  nu_im1 = d__vectors + N;
  nu_ip1 = d__vectors + 2*N;
  w_i    = d__vectors + 3*N;
  w_im1  = d__vectors + 4*N;
  w_im2  = d__vectors + 5*N;
  cublasCreate(&handle);
}
@ \end{itemize}
@ 
The structure parameters are
<<parameters>>=
float *d__vectors,
  *q, *x, *r, *p,
  *nu_i, *nu_im1, *nu_ip1,
  *w_i, *w_im1, *w_im2;
float rnorm, mean_time_per_iteration;
int N;
stopwatch tid;
cublasHandle_t handle;
cublasStatus_t status;
@
Memory is freed with
<<cleanup>>= 
void iterativeSolvers::cleanup(void)
{
  printf("\n@(CEO)>iterativeSolvers: freeing memory!\n");
  HANDLE_ERROR( cudaFree( d__vectors ) );
  cublasDestroy(handle);
}
@ 

\subsection{Conjugate gradient}
\label{sec:conjugate-gradient}

The next routine implements the conjugate gradient iterative method:
<<conjugate gradient>>=
  void iterativeSolvers::cg(float *x, BTBT* A, float *b, int max_it, float* x0)
{
  float alpha, beta, gamma;
  int k;
  <<conjugate gradient init>>
  tid.tic();
  for (k=0;k<max_it;k++) {
    //printf("CG IT=%d - ",k);
    <<conjugate gradient loop part 1>>
      if (k<(max_it-1)) {
	  <<conjugate gradient loop part 2>>
      }
  }    
  tid.toc(&mean_time_per_iteration);
  mean_time_per_iteration /= max_it;
}
@ 
The conjugate gradient algorithm is written:
\begin{itemize}
\item initialization:
  \begin{enumerate}
  \item $r=Ax_0$
<<conjugate gradient init>>=
    A->MVM(r , x0);
@ \item $r_0 = b - r_0$
<<conjugate gradient init>>=
beta = -1;
cublasSscal(handle, N, &beta, r, 1);
alpha = 1;
cublasSaxpy(handle, N, &alpha, b, 1, r, 1);
@ \item $p_0=r_0$
<<conjugate gradient init>>=
cublasScopy(handle, N, r, 1, p, 1);
@ \end{enumerate}
\item loop:
  \begin{enumerate}
  \item $q=Ap_k$
<<conjugate gradient loop part 1>>=
    A->MVM(q , p);
@ \item $\gamma=r_k^Tr_k$
<<conjugate gradient loop part 1>>=
cublasSdot(handle, N, r, 1, r, 1, &gamma);
//printf("r norm=%6.2E\n",sqrt(gamma));
@ \item $\left\| r \right\|_2=\sqrt\gamma$
<<conjugate gradient loop part 1>>=
rnorm = sqrt(gamma);
if (gamma==0) break;
@ \item $\alpha_k = {\displaystyle \gamma\over p_K^Tq}$
<<conjugate gradient loop part 1>>=
cublasSdot(handle, N, p, 1, q, 1, &alpha);
alpha = gamma/alpha;
@ \item $x_{k+1} = x_k + \alpha_kp_k$
<<conjugate gradient loop part 1>>=
cublasSaxpy(handle, N, &alpha, p, 1, x, 1);
@ \item $r_{k+1} = r_k - \alpha_kq$
<<conjugate gradient loop part 2>>=
alpha = -alpha;
cublasSaxpy(handle, N, &alpha, q, 1, r, 1);
@ \item $\beta_k = {\displaystyle r_{k+1}^Tr_{k+1} \over \gamma} $
<<conjugate gradient loop part 2>>=
cublasSdot(handle, N, r, 1, r, 1, &beta);
beta = beta/gamma;
@ \item $p_{k+1} = r_{k+1} + \beta_k p_k$
<<conjugate gradient loop part 2>>=
cublasSscal(handle, N, &beta, p, 1);
alpha = 1;
cublasSaxpy(handle, N, &alpha, r, 1, p, 1);
@  \end{enumerate}
 \end{itemize}

 \subsection{MINRES}
 \label{sec:minres}

\subsubsection{Vorst implementation}
\label{sec:vorst-implementation}

The MINRES iterative solver method is implemented in the next routine:
<<MINRES (Vorst)>>=
  void iterativeSolvers::minres_vorst(float *x, BTBT* A, float *b, int max_it, float* x0)
{
  float alpha, beta, gamma_i, gamma_im1, sigma_i, sigma_im1,
    eta, delta, rho1, rho2, rho3;
  int k;
  <<MINRES (Vorst) init>>
      //  tid.tic();
  for (k=0;k<max_it;k++) {
    //printf("MINRES IT=%d - ",k);
    <<MINRES (Vorst) loop>>
  }   
  //  tid.toc(&mean_time_per_iteration);
  //  mean_time_per_iteration /= max_it;
}
<<MINRES (Vorst) with residue>>=
  void iterativeSolvers::minres_vorst(float *x, float *res, BTBT* A, float *b, int max_it, float* x0)
{
  float alpha, beta, gamma_i, gamma_im1, sigma_i, sigma_im1,
    eta, delta, rho1, rho2, rho3, res_cst, result;
  int k;
  float *residual;
  HANDLE_ERROR( cudaMalloc( (void**)&residual, sizeof(float)*N ) );
  <<MINRES (Vorst) init>>
  tid.tic();
  for (k=0;k<max_it;k++) {
    //printf("MINRES IT=%d - ",k);
    <<MINRES (Vorst) loop>>
    A->MVM(residual , x);
    res_cst = -1;
    cublasSscal(handle, N, &res_cst, residual, 1);
    res_cst = 1;
    cublasSaxpy(handle, N, &res_cst, b, 1, residual, 1);
    cublasSnrm2(handle, N, residual, 1, &result);
    res[k] = result;
    printf("(%3d): Residue norm: %.2E\n",k,result);
  }   
  tid.toc(&mean_time_per_iteration);
  mean_time_per_iteration /= max_it;
  HANDLE_ERROR( cudaFree( residual) );
}
@ 
The MINRES algorithm is written:
\begin{itemize}
\item initialization:
  \begin{enumerate}
  \item $r=Ax_0$
<<MINRES (Vorst) init>>=
A->MVM(nu_i , x0);
@ \item $r_0 = b - r_0$
<<MINRES (Vorst) init>>=
beta = -1;
cublasSscal(handle, N, &beta, nu_i, 1);
alpha = 1;
cublasSaxpy(handle, N, &alpha, b, 1, nu_i, 1);
#ifdef MINRES_DEBUG
cublasSnrm2(handle, N, b, 1, &beta);
printf("beta=%.2E\n",beta);
#endif
@ \item $\beta_1=\left\| \nu_1 \right\|_2=\left\| r \right\|_2$
<<MINRES (Vorst) init>>=
  //cublasSnrm2(handle, N, nu_i, 1, &beta);
cublasSdot(handle, N, nu_i, 1, nu_i, 1, &beta);
beta = sqrtf(beta);
rnorm = beta;
#ifdef MINRES_DEBUG
printf("beta=%.2E\n",beta);
#endif
@ \item $\eta=beta_1$, $\gamma_1=\gamma_0=1$, $\sigma_1=\sigma_0=0$, $\nu_0=0$
<<MINRES (Vorst) init>>=
eta = beta;
gamma_i = gamma_im1 = 1;
sigma_i = sigma_im1 = 0;
@ \item $\nu_0=0$, $w_0=w_{-1}=0$
<<MINRES (Vorst) init>>=
HANDLE_ERROR( cudaMemset(nu_im1, 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(w_im1 , 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(w_im2 , 0, sizeof(float)*N ) );
@ \end{enumerate}
\item loop:
  \begin{enumerate}
  \item $\nu_i = \nu_i / \beta_i$
<<MINRES (Vorst) loop>>=
rho1 = 1/beta;
cublasSscal(handle, N, &rho1, nu_i, 1);
@ \item $\nu_{i+1}= A\nu_i$
<<MINRES (Vorst) loop>>=
A->MVM(nu_ip1 , nu_i);
@ \item $\alpha_i = \nu_i^T\nu_{i+1}$
<<MINRES (Vorst) loop>>=
cublasSdot(handle, N, nu_i, 1, nu_ip1, 1, &alpha);
#ifdef MINRES_DEBUG
printf("alpha=%.2E\n",alpha);
#endif
@ \item $\nu_{i+1} = \nu_{i+1} - \alpha_i\nu_i - \beta_i\nu_{i-1}$
<<MINRES (Vorst) loop>>=
alpha = -alpha;
beta  = -beta;
cublasSaxpy(handle, N, &alpha, nu_i, 1, nu_ip1, 1);
cublasSaxpy(handle, N, &beta, nu_im1, 1, nu_ip1, 1);
alpha = -alpha;
beta  = -beta;
@ \item $\delta = \gamma_i\alpha_i - \gamma_{i-1}\sigma_i\beta_i$
<<MINRES (Vorst) loop>>=
delta = gamma_i*alpha - gamma_im1*sigma_i*beta;
#ifdef MINRES_DEBUG
printf("delta=%.2E\n",delta);
#endif
@ \item $\rho_2 = \sigma_i\alpha_i + \gamma_{i-1}\gamma_i\beta_i$
<<MINRES (Vorst) loop>>=
rho2 = sigma_i*alpha + gamma_im1*gamma_i*beta;
#ifdef MINRES_DEBUG
printf("rho2=%.2E\n",rho2);
#endif
@ \item $\rho_3 = \sigma_{i-1}\beta_i$
<<MINRES (Vorst) loop>>=
rho3 = sigma_im1*beta;
#ifdef MINRES_DEBUG
printf("rho3=%.2E\n",rho3);
#endif
@ \item $\beta_{i+1}=\left\| \nu_{i+1} \right\|_2$
<<MINRES (Vorst) loop>>=
  //cublasSnrm2(handle, N, nu_ip1, 1, &beta);
cublasSdot(handle, N, nu_ip1, 1, nu_ip1, 1, &beta);
beta = sqrtf(beta);
#ifdef MINRES_DEBUG
printf("beta=%.2E\n",beta);
#endif
@ \item $\rho_1 = \sqrt{\delta^2+\beta_{i+1}^2}$
<<MINRES (Vorst) loop>>=
rho1 = hypotf(delta,beta);
#ifdef MINRES_DEBUG
printf("rho1=%.2E\n",rho1);
#endif
@ \item $\gamma_{i+1} = \delta / \rho_1$
<<MINRES (Vorst) loop>>=
gamma_im1 = gamma_i;
gamma_i = delta/rho1;
#ifdef MINRES_DEBUG
printf("gamma_im1=%.2E\n",gamma_im1);
printf("gamma_i=%.2E\n",gamma_i);
#endif
@ \item $\sigma_{i+1} = \beta_{i+1} / \rho_1$
<<MINRES (Vorst) loop>>=
sigma_im1 = sigma_i;
sigma_i = beta/rho1;
#ifdef MINRES_DEBUG
printf("sigma_im1=%.2E\n",sigma_im1);
printf("sigma_i=%.2E\n",sigma_i);
#endif
@ \item $w_i = (\nu_i-\rho_3w_{i-2}-\rho_2w_{i-1})/\rho_1$
<<MINRES (Vorst) loop>>=
cublasScopy(handle, N, nu_i, 1, w_i, 1);
rho1 = 1.0/rho1;
cublasSscal(handle, N, &rho1, w_i, 1);
rho3 = -rho3*rho1;
cublasSaxpy(handle, N, &rho3, w_im2, 1, w_i, 1);
rho2 = -rho2*rho1;
cublasSaxpy(handle, N, &rho2, w_im1, 1, w_i, 1);
@ \item $x_i = x_{i-1} + \gamma_{i+1}\eta w_i$
<<MINRES (Vorst) loop>>=
rho1 = gamma_i*eta;
cublasSaxpy(handle, N, &rho1, w_i, 1, x, 1);
@ \item $\left\| r_i \right\|_2 = \left| \sigma_{i+1} \right|\left\| r_{i-1} \right\|_2 $
<<MINRES (Vorst) loop>>=
rnorm = rnorm*fabs(sigma_i);
//printf("r norm=%6.2E\n",rnorm);
@ \item $\eta = \sigma_{i+1}\eta$
<<MINRES (Vorst) loop>>=
eta = -sigma_i*eta;
#ifdef MINRES_DEBUG
printf("eta=%.2E\n",eta);
#endif
@ \item
\begin{eqnarray}
  \label{eq:1}
  \nu_{i-1} &\leftarrow& \nu_i \nonumber\\
  \nu_{i} &\leftarrow& \nu_{i+1} \nonumber\\
  w_{i-2} &\leftarrow& w_{i-1} \nonumber\\
  w_{i-1} &\leftarrow& \nu_{i} \nonumber
\end{eqnarray}
<<MINRES (Vorst) loop>>=
cublasScopy(handle, N, nu_i  , 1, nu_im1, 1);
cublasScopy(handle, N, nu_ip1, 1, nu_i  , 1);
cublasScopy(handle, N, w_im1 , 1, w_im2 , 1);
cublasScopy(handle, N, w_i   , 1, w_im1 , 1);
if (beta==0) break;
@  \end{enumerate}
 \end{itemize}

 \subsubsection{Choi implementation}
 \label{sec:choi-implementation}

<<MINRES (Choi)>>=
  void iterativeSolvers::minres_choi(float *x, BTBT* A, float *b, int max_it, float* x0)
{
  float a, alpha, beta, gamma, delta1, delta2,  phi, tau, c, s, epsilon;
  int k;
  <<MINRES (Choi) init>>
  tid.tic();
  for (k=0;k<max_it;k++) {
    //printf("MINRES IT=%d - ",k);
    <<MINRES (Choi) loop>>
  }    
  tid.toc(&mean_time_per_iteration);
  mean_time_per_iteration /= max_it;
}
@ 
The MINRES algorithm is written:
\begin{itemize}
\item initialization:
  \begin{enumerate}
  \item $\beta_1=\left\| b \right\|_2$
<<MINRES (Choi) init>>=
cublasSnrm2(handle, N, b, 1, &beta);
@ \item $\nu_0 = 0$
<<MINRES (Choi) init>>=
HANDLE_ERROR( cudaMemset(nu_im1, 0, sizeof(float)*N ) );
@ \item $\nu_1 = b/\beta_1$
<<MINRES (Choi) init>>=
HANDLE_ERROR( cudaMemset(nu_i, 0, sizeof(float)*N ) );
a = 1/beta;
cublasSaxpy(handle, N, &a, b, 1, nu_i, 1);
@ \item $\phi=\tau=\beta_1$, $\delta^{(1)}_1=0$, $c_0=-1$, $s_0=0$
<<MINRES (Choi) init>>=
phi= tau = beta;
delta1 = 0;
c = -1;
s = 0;
@ \item $d_0=d_{-1}=0$
<<MINRES (Choi) init>>=
HANDLE_ERROR( cudaMemset(w_im1 , 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(w_im2 , 0, sizeof(float)*N ) );
@ \end{enumerate}
\item loop:
  \begin{enumerate}
  \item Lanczos step
<<MINRES (Choi) loop>>=
lanczos_step(&alpha, &beta, nu_ip1, A, nu_i, nu_im1, 0);
@ \item $\delta^{(2)}_k=c_{k-1}\delta^{(1)}_k + s_{k-1}\alpha_k$
<<MINRES (Choi) loop>>=
delta2 = c*delta1 + s*alpha;
@ \item $\gamma^{(1)}_k=s_{k-1}\delta^{(1)}_k - c_{k-1}\alpha_k$
<<MINRES (Choi) loop>>=
gamma = s*delta1 - c*alpha;
@ \item $\epsilon_k = s_{k-1}\beta_{k+1}$
<<MINRES (Choi) loop>>=
epsilon = s*beta;
@ \item $\delta^{(1)}_k = -c_{k-1}\beta_{k+1}$
<<MINRES (Choi) loop>>=
delta1 = -c*beta;
@ \item sym--ortho
<<MINRES (Choi) loop>>=
a = gamma;
sym_ortho(&c, &s, &gamma, &a, &beta);
@ \item $\tau_k = c_k\phi_k$
<<MINRES (Choi) loop>>=
tau = c*phi;
@ \item $\phi_k = s_k\phi_{k-1}$
<<MINRES (Choi) loop>>=
phi = s*phi;
//printf("r norm=%6.2E\n",phi);
@ \item $\left\| r \right\|_2 = \phi_k$
<<MINRES (Choi) loop>>=
rnorm = phi;
@ \item $d_k = (\nu_k-\delta^{(2)}_k d_{k-1}-\epsilon_k d_{k-2})/\gamma_k$
<<MINRES (Choi) loop>>=
if (gamma==0) break;
cublasScopy(handle, N, nu_i, 1, w_i, 1);
a = 1/gamma;
cublasSscal(handle, N, &a, w_i, 1);
a = -delta2/gamma;
cublasSaxpy(handle, N, &a, w_im1, 1, w_i, 1);
a = -epsilon/gamma;
cublasSaxpy(handle, N, &a, w_im2, 1, w_i, 1);
@ \item $x_k = x_{k-1} + \tau d_k$
<<MINRES (Choi) loop>>=
cublasSaxpy(handle, N, &tau, w_i, 1, x, 1);
@ \item 
\begin{eqnarray}
  \label{eq:1}
  \nu_{i-1} &\leftarrow& \nu_i \nonumber\\
  \nu_{i} &\leftarrow& \nu_{i+1} \nonumber\\
  d_{i-2} &\leftarrow& d_{i-1} \nonumber\\
  d_{i-1} &\leftarrow& d_{i} \nonumber
\end{eqnarray}
<<MINRES (Choi) loop>>=
cublasScopy(handle, N, nu_i  , 1, nu_im1, 1);
cublasScopy(handle, N, nu_ip1, 1, nu_i  , 1);
cublasScopy(handle, N, w_im1 , 1, w_im2 , 1);
cublasScopy(handle, N, w_i   , 1, w_im1 , 1);
@  \end{enumerate}
 \end{itemize}
@ 
The sym--ortho function is:
<<sym-ortho algorithm>>=
void iterativeSolvers::sym_ortho(float *c,float *s,float *r,float *a,float *b) {
  float abs_a, abs_b, tau;
  abs_a = fabs(*a);
  abs_b = fabs(*b);
  if (*b==0) {
    *s = 0;
    *r = abs_a;
    *c = (*a==0) ? 1 : sign(*a);
  } else if (*a==0) {
    *c = 0;
    *s = sign(*b);
    *r = abs_b;
  } else if (abs_b>abs_a) {
    tau = *a/(*b);
    *s = sign(*b)/sqrtf(1+tau*tau);
    *c = *s*tau;
    *r = *b/(*s);
  } else if (abs_a>abs_b) {
    tau = *b/(*a);
    *c = sign(*a)/sqrtf(1+tau*tau);
    *s = *c*tau;
    *r = *a/(*c);
  }
}
@ 
The Lanzcos step is:
<<Lanczos step>>=
void iterativeSolvers::lanczos_step(float *alpha, float *beta, float *nu_kp1,
		  BTBT *A, float *nu_k, float *nu_km1, float sigma)
{
  float a;
  A->MVM(nu_kp1, nu_k);
  if (sigma!=0) {
    a = -sigma;
    cublasSaxpy(handle, N, &a, nu_k, 1, nu_kp1, 1);
  }
  cublasSdot(handle, N, nu_k, 1, nu_kp1, 1, alpha);
  a = -(*alpha);
  cublasSaxpy(handle, N, &a, nu_k, 1, nu_kp1, 1);
  a = -(*beta);
  cublasSaxpy(handle, N, &a, nu_km1, 1, nu_kp1, 1);
  cublasSnrm2(handle, N, nu_kp1, 1, beta);
  if (beta!=0) {
    a = 1/(*beta);
    cublasSscal(handle, N, &a, nu_kp1, 1);
  }
}

@
\subsection{Tests}
\label{sec:tests}

The test routine is:
<<iterativeSolvers.bin>>=

#include "cuda_profiler_api.h"

#ifndef __CEO_H__
#include "ceo.h"
#endif
#ifndef __SOURCE_H__
#include "source.h"
#endif
#ifndef __ATMOSPHERE_H__
#include "atmosphere.h"
#endif
#ifndef __IMAGING_H__
#include "imaging.h"
#endif
#ifndef __CENTROIDING_H__
#include "centroiding.h"
#endif
#ifndef __AASTATS_H__
#include "aaStats.h"
#endif
#ifndef __BTBT_H__
#include "BTBT.h"
#endif
#include "iterativeSolvers.h"

   //#define N 10

   <<PA input>>

__global__ void fill(float *data, int n_data, float value)
{
  int k = blockIdx.x * blockDim.x + threadIdx.x;
  if (k<n_data)
    data[k] = value;
}

// Solving Ax=b
int main(int argc,char *argv[]) {
<<complete test>>
}
@
In the following a simple test is performed:
<<simple test I>>=
/*
  The test requires to replace the MVM with MVM (Test 1) 
  in the code chunk BTBT.cu in the file BTBT.nw
*/
float *d__x, *d__b;
HANDLE_ERROR( cudaMalloc((void**)&d__x, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );
HANDLE_ERROR( cudaMalloc((void**)&d__b, sizeof(float)*N ) );
float b[N] = {1,2,3,4,5,6,7,8,9,10};
HANDLE_ERROR( cudaMemcpy( d__b, b, N*sizeof(float), cudaMemcpyHostToDevice) );

BTBT A;
A.setup(N);

iterativeSolvers iSolve;
/* iSolve.cg_setup(N); */
/* iSolve.cg(d__x, &A, d__b, 5, d__x); */
iSolve.minres_setup(N);
iSolve.minres(d__x, &A, d__b, 5, d__x);
iSolve.cleanup();

float *x;
x = (float *)malloc(sizeof(float)*N);
HANDLE_ERROR( cudaMemcpy( x, d__x, N*sizeof(float), cudaMemcpyDeviceToHost) );

for (int k=0;k<N;k++)
  printf("(%d) %5.1f\n",k,x[k]);

HANDLE_ERROR( cudaFree( d__x ) );
HANDLE_ERROR( cudaFree( d__b ) );
free(x);
@ 
<<simple test II>>=
/*
  The test requires to replace the MVM with MVM (Test 2) 
  in the code chunk BTBT.cu in the file BTBT.nw
*/
float *x;
x = (float *)malloc(sizeof(float)*N);
float *d__x, *d__b;
HANDLE_ERROR( cudaMalloc((void**)&d__x, sizeof(float)*N ) );
HANDLE_ERROR( cudaMalloc((void**)&d__b, sizeof(float)*N ) );

for (int k=0;k<N;k++) {
  x[k] = 1.0 + (float)rand();
  printf("(%d) %.3E\n",k,x[k]);
 }
HANDLE_ERROR( cudaMemcpy( d__b, x, N*sizeof(float), cudaMemcpyHostToDevice ) );

BTBT A;
A.setup(N);
/* A.MVM(d__b,d__x); */
/* HANDLE_ERROR( cudaMemcpy( x, d__b, N*sizeof(float), cudaMemcpyDeviceToHost) ); */

/* for (int k=0;k<N;k++) */
/*   printf("(%d) %5.1f\n",k,x[k]); */

fill LLL (1+N/16),16 RRR (d__x, N, 0);

iterativeSolvers iSolve;
/* iSolve.cg_setup(N); */
/* iSolve.cg(d__x, &A, d__b, 5, d__x); */
iSolve.minres_setup(N);
iSolve.minres(d__x, &A, d__b, 5, d__x);
iSolve.cleanup();

HANDLE_ERROR( cudaMemcpy( x, d__x, N*sizeof(float), cudaMemcpyDeviceToHost) );

for (int k=0;k<N;k++)
  printf("(%d) %5.1f\n",k,x[k]);

HANDLE_ERROR( cudaFree( d__x ) );
HANDLE_ERROR( cudaFree( d__b ) );
free(x);
@ 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TEST ZONE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This is the comprehensive suite of tests.
There are 3 tests available: [[[main test]]], [[convergence test]] and [[statistics test]].
$r_0$ is set to 15cm per default but can be override to any other value after the variable declaration.
<<complete test>>=
<<declaration>>
  r0 = d;
<<initialization>>
<<hot convergence test>>
<<cleanup and free memory>>
@ 
In the following, all the variables are declared and set to their default or user given values.
<<declaration>>=
int N = _N_LENSLET_*2, NP, PS_N_PX, PS_E_N_PX, i, j ,k, osf;
float *d__x, *d__b, *b, *d__ce, *d__phase_est, *phase_screen_est, *x;
float D, d, delta, delta_e, wf_rms, phase2nm, slopes2Angle, cxy0, r0=0.15;
int *idx, *d__idx;
char *d__mask;

if (argc != 3) {
  printf("Usage: %s D CG or MINRES\n",argv[0]);
  return 1;
 }
char solver[10];
sprintf(solver,"%s",argv[2]);
printf("\n___ SOLVER: %s ___\n",solver);

D = atof(argv[1]);
d = D/N_SIDE_LENSLET;
delta = d/_N_PX_PUPIL_;
osf = 2;
delta_e = d/osf;

NP = osf*N_SIDE_LENSLET+1;
PS_E_N_PX = NP;
PS_E_N_PX *= PS_E_N_PX;
PS_N_PX = N_SIDE_LENSLET*_N_PX_PUPIL_;

printf("\nd    =%.4f\n",d);
printf("\ndelta=%.4f\n",delta);
printf("\ndelta_e=%.4f\n",delta_e);

source src, *d__src;
atmosphere atm;
imaging lenslet_array;
centroiding cog;
aaStats aa;
BTBT aaCov;
paStats pa;
BTBT paCov;
iterativeSolvers iSolve;
stats S;
S.setup();
stopwatch tid;
@ 
Then all the components are initialized and the dynamic arrays are allocated.
<<initialization>>=
src.setup(ARCSEC(0) , 0, INFINITY);
HANDLE_ERROR( cudaMalloc( (void**)&d__src, sizeof(source)*_N_SOURCE_ ) );
HANDLE_ERROR( cudaMemcpy( d__src, &src,
			  sizeof(source)*_N_SOURCE_ ,
			  cudaMemcpyHostToDevice ) );
        
// Single layer turbulence profile
/*
float altitude[] = {0},
  xi0[] = {1},
  wind_speed[] = {10},
  wind_direction[] = {0};
*/

// GMT 7 layers turbulence profile
float altitude[] = {25, 275, 425, 1250, 4000, 8000, 13000},
              xi0[] = {0.1257, 0.0874, 0.0666, 0.3498, 0.2273, 0.0681, 0.0751},
              wind_speed[] = {5.6540, 5.7964, 5.8942, 6.6370, 13.2925, 34.8250, 29.4187},
              wind_direction[] = {0.0136, 0.1441, 0.2177, 0.5672, 1.2584, 1.6266, 1.7462};

// Atmosphere
atm.setup(r0,30,altitude,xi0,wind_speed,wind_direction);
//atm.reset();

phase2nm = 1E9*atm.wavelength/2/PI;
slopes2Angle = (atm.wavelength/2/d);

// SH WFS
lenslet_array.setup();

// Centroid
cog.setup();

// covariances
aa.setup(N_SIDE_LENSLET,&atm,d);
printf("\n AA variance [rd^2]: %.3E\n",aa.variance());
aaCov.setup(2,2,N_SIDE_LENSLET,aa.d__cov);

pa.setup(N_SIDE_LENSLET,osf,&atm,d);
paCov.setup(1,2,NP,pa.d__cov);

HANDLE_ERROR( cudaMalloc((void**)&d__mask, sizeof(char)*_N_LENSLET_ ) );
dim3 blockDim_vl(16,16);
dim3 gridDim_vl(N_SIDE_LENSLET/16+1,N_SIDE_LENSLET/16+1);
valid_lenslet LLL gridDim_vl,blockDim_vl RRR (d__mask,N_SIDE_LENSLET*_N_PX_PUPIL_,0.5);

HANDLE_ERROR( cudaMalloc( (void**)&d__phase_est           , sizeof(float)*PS_E_N_PX ) );
@ 
\subsubsection{Main test}
\label{sec:main-test}

The [[main test]] is simply a wavefront reconstruction routine.
<<main test>>=
atm.get_phase_screen(delta,PS_N_PX,delta,PS_N_PX,d__src,0);

wf_rms = phase2nm*S.std(d__src->phase, _N_PIXEL_);
printf("\n WF RMS: %7.2fnm\n",wf_rms);

float phase_screen[_N_PIXEL_];
HANDLE_ERROR( cudaMemcpy( phase_screen, d__src->phase,
			  sizeof(float)*_N_PIXEL_,
			  cudaMemcpyDeviceToHost ) );
FILE *fid;
fid = fopen("phaseScreen.bin","wb");
fwrite(phase_screen,sizeof(float),_N_PIXEL_,fid);
fclose(fid);

float *d__phase_screen_low_res;
HANDLE_ERROR( cudaMalloc( (void**)&d__phase_screen_low_res, sizeof(float)*PS_E_N_PX ) );
atm.get_phase_screen(d__phase_screen_low_res,delta_e,NP,delta_e,NP,d__src,0);

float *phase_screen_low_res;
phase_screen_low_res = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMemcpy( phase_screen_low_res, d__phase_screen_low_res,
			  sizeof(float)*PS_E_N_PX,
			  cudaMemcpyDeviceToHost ) );
fid = fopen("phaseScreenLowRes.bin","wb");
fwrite(phase_screen_low_res,sizeof(float),PS_E_N_PX,fid);
fclose(fid);

<<wavefront sensing>>
  //atm.get_phase_screen_gradient(cog.d__cx,cog.d__cy,N_SIDE_LENSLET,d,d__src,0);

// Iterative solver
  //iSolve.cg_setup(N);
iSolve.minres_setup(N);

<<linear solver (prep.)>>
<<MVM (prep.)>>

char filename[100];
/* for (k=1;k<6;k++) { */
k = 400;

sprintf(filename,"MINRES_phaseEst_%d.bin",k);
//sprintf(filename,"CG_phaseEst_%d.bin",k);
HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );

tid.tic();

//cudaProfilerStart();
iSolve.minres_vorst(d__x, &aaCov, cog.d__c, k, d__x);
//iSolve.cg(d__x, &aaCov, cog.d__c, k, d__x);
//cudaProfilerStop();

<<MVM>>

tid.toc("WAVEFRONT ESTIMATION");

<<MVM (wrap.)>>

printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);

/* } */
HANDLE_ERROR( cudaFree( d__b ) );
free(b);
@ 
\subsubsection{Convergence test}
\label{sec:convergence-test}

The [[convergence test]] reconstructs the wavefront with both iterative solver MINRES and CG at each iteration step.
<<convergence test>>=
atm.get_phase_screen(delta,PS_N_PX,delta,PS_N_PX,d__src,0);

wf_rms = phase2nm*S.std(d__src->phase, _N_PIXEL_);
printf("\n WF RMS: %7.2fnm\n",wf_rms);

float phase_screen[_N_PIXEL_];
HANDLE_ERROR( cudaMemcpy( phase_screen, d__src->phase,
			  sizeof(float)*_N_PIXEL_,
			  cudaMemcpyDeviceToHost ) );
FILE *fid;
fid = fopen("phaseScreen.bin","wb");
fwrite(phase_screen,sizeof(float),_N_PIXEL_,fid);
fclose(fid);

float *d__phase_screen_low_res;
HANDLE_ERROR( cudaMalloc( (void**)&d__phase_screen_low_res, sizeof(float)*PS_E_N_PX ) );
atm.get_phase_screen(d__phase_screen_low_res,delta_e,NP,delta_e,NP,d__src,0);

float *phase_screen_low_res;
phase_screen_low_res = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMemcpy( phase_screen_low_res, d__phase_screen_low_res,
			  sizeof(float)*PS_E_N_PX,
			  cudaMemcpyDeviceToHost ) );
char filename[100];
sprintf(filename,"CVGCE_phaseScreenLowRes_%03d.bin",N_SIDE_LENSLET);
fid = fopen(filename,"wb");
fwrite(phase_screen_low_res,sizeof(float),PS_E_N_PX,fid);
fclose(fid);

<<wavefront sensing (masked)>>

<<linear solver (prep.)>>
aaCov.mask = d__mask;
<<MVM (prep.)>>


sprintf(filename,"CVGCE_%s_phaseEst_200_%03d.bin",solver,N_SIDE_LENSLET);

fid = fopen(filename,"wb");
phase_screen_est     = (float*)malloc(sizeof(float)*PS_E_N_PX);
// CG
if (strcmp(solver,"CG")==0) {
  iSolve.cg_setup(N);
  for (k=1;k<=200;k++) {

    printf("\n______ ITERATION #: %03d ______\n",k);

    HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );

    tid.tic();

    iSolve.cg(d__x, &aaCov, cog.d__c, k, d__x);
    
    <<MVM>>

    tid.toc("WAVEFRONT ESTIMATION");

    HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
			      sizeof(float)*PS_E_N_PX,
			      cudaMemcpyDeviceToHost ) );
    fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);

    printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
    printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);
    printf("\n-------------------------------\n");

  }
	 }
// MINRES
if (strcmp(solver,"MINRES")==0) {
  iSolve.minres_setup(N);
  for (k=1;k<=200;k++) {

    printf("\n______ ITERATION #: %03d ______\n",k);

    HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );

    tid.tic();

    iSolve.minres_vorst(d__x, &aaCov, cog.d__c, k, d__x);

    <<MVM>>

    tid.toc("WAVEFRONT ESTIMATION");

    HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
			      sizeof(float)*PS_E_N_PX,
			      cudaMemcpyDeviceToHost ) );
    fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);

    printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
    printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);
    printf("\n-------------------------------\n");

  }
	 }

fclose(fid);
HANDLE_ERROR( cudaFree( d__b ) );
free(b);
@ 
\subsubsection{Hot convergence test}
\label{sec:hot-convergence-test}


The [[hot convergence test]] reconstructs the wavefront with MINRES re-using the previous estimate as starting point for the next one.
<<hot convergence test>>=
FILE *fid;
  /*
atm.get_phase_screen(delta,PS_N_PX,delta,PS_N_PX,d__src,0);

wf_rms = phase2nm*S.std(d__src->phase, _N_PIXEL_);
printf("\n WF RMS: %7.2fnm\n",wf_rms);

float phase_screen[_N_PIXEL_];
HANDLE_ERROR( cudaMemcpy( phase_screen, d__src->phase,
			  sizeof(float)*_N_PIXEL_,
			  cudaMemcpyDeviceToHost ) );
fid = fopen("phaseScreen.bin","wb");
fwrite(phase_screen,sizeof(float),_N_PIXEL_,fid);
fclose(fid);
*/

float *d__phase_screen_low_res;
HANDLE_ERROR( cudaMalloc( (void**)&d__phase_screen_low_res, sizeof(float)*PS_E_N_PX ) );
float *phase_screen_low_res;
phase_screen_low_res = (float*)malloc(sizeof(float)*PS_E_N_PX);

char filename[100];
sprintf(filename,"CVGCE_phaseScreenLowRes_%03d.bin",N_SIDE_LENSLET);
FILE *fid_phaseScreenLowRes;
fid_phaseScreenLowRes = fopen(filename,"wb");

<<linear solver (prep.)>>
aaCov.mask = d__mask;
<<MVM (prep.)>>

sprintf(filename,"CVGCE_%s_phaseEst_200_%03d.bin",solver,N_SIDE_LENSLET);
fid = fopen(filename,"wb");

phase_screen_est     = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );
float tau=0;
int solv_it;
solv_it = 60;

// MINRES
if (strcmp(solver,"MINRES")==0) {
  iSolve.minres_setup(N);
  for (k=1;k<=200;k++) {

    printf("\n______ ITERATION #: %03d ______\n",k);
    
    tau = 2e-3*(k-1);
    atm.get_phase_screen(d__phase_screen_low_res,delta_e,NP,delta_e,NP,d__src,tau);
    HANDLE_ERROR( cudaMemcpy( phase_screen_low_res, d__phase_screen_low_res,
    			  sizeof(float)*PS_E_N_PX,
    			  cudaMemcpyDeviceToHost ) );
    fwrite(phase_screen_low_res,sizeof(float),PS_E_N_PX,fid_phaseScreenLowRes);

    atm.get_phase_screen_gradient(cog.d__cx,cog.d__cy,N_SIDE_LENSLET,d__mask,d,d__src,tau);
    /* if (k>10) */
    /*   solv_it = 0; */
    
       tid.tic();

    iSolve.minres_vorst(d__x, &aaCov, cog.d__c, solv_it, d__x);

    <<MVM>>

	  tid.toc("WAVEFRONT ESTIMATION");

    HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
    			      sizeof(float)*PS_E_N_PX,
    			      cudaMemcpyDeviceToHost ) );
    fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);

    printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
    printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);
    printf("\n-------------------------------\n");

  }
 }

fclose(fid_phaseScreenLowRes);
fclose(fid);
//HANDLE_ERROR( cudaFree( d__b ) );
//free(b);
@
\subsubsection{Statistics test}
\label{sec:statistics-test}

The [[statistics test]] reconstructs the wavefront n times, each time a new statistically independent phase screen is generated.
<<statistics test>>=
float *d__phase_screen_low_res;
HANDLE_ERROR( cudaMalloc( (void**)&d__phase_screen_low_res, sizeof(float)*PS_E_N_PX ) );

float *phase_screen_low_res;
phase_screen_low_res = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMemcpy( phase_screen_low_res, d__phase_screen_low_res,
			  sizeof(float)*PS_E_N_PX,
			  cudaMemcpyDeviceToHost ) );

<<linear solver (prep.)>>
aaCov.mask = d__mask;
<<<<MVM (prep.)>>

FILE *fid0, *fid;
char filename[100];
sprintf(filename,"STATS_phaseScreenLowRes_%03d.bin",N_SIDE_LENSLET);
fid0 = fopen(filename,"wb");

int nSample = 200, nIt = 200;
sprintf(filename,"STATS_%s_phaseEst_%03d_%03d.bin",solver,nSample,N_SIDE_LENSLET);
fid = fopen(filename,"wb");

phase_screen_est     = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMalloc( (void**)&d__phase_est           , sizeof(float)*PS_E_N_PX ) );
// CG
if (strcmp(solver,"CG")==0) {
  iSolve.cg_setup(N);
  for (k=1;k<=nSample;k++) {

    printf("\n______ ITERATION #: %03d ______\n",k);

    <<atmosphere propagation and wavefront sensing>>

    HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );

    tid.tic();

    iSolve.cg(d__x, &aaCov, cog.d__c, nIt, d__x);
    
    <<MVM>>

    tid.toc("WAVEFRONT ESTIMATION");

    HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
    			      sizeof(float)*PS_E_N_PX,
    			      cudaMemcpyDeviceToHost ) );
    fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);

    printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
    printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);
    printf("\n-------------------------------\n");

    atm.reset();
  }
	 }
// MINRES
if (strcmp(solver,"MINRES")==0) {
  iSolve.minres_setup(N);
  for (k=1;k<=nSample;k++) {

    printf("\n______ ITERATION #: %03d ______\n",k);

    <<atmosphere propagation and wavefront sensing>>

    HANDLE_ERROR( cudaMemset(d__x, 0, sizeof(float)*N ) );

    tid.tic();

    iSolve.minres_vorst(d__x, &aaCov, cog.d__c, nIt, d__x);

    <<MVM>>

    tid.toc("WAVEFRONT ESTIMATION");

    HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
			      sizeof(float)*PS_E_N_PX,
			      cudaMemcpyDeviceToHost ) );
    HANDLE_ERROR( cudaThreadSynchronize() );
    fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);
    printf("\n Data saved to file!\n");

    printf("\nSolver residue norm           : %.2E\n",iSolve.rnorm);
    printf("\nSolver mean time per iteration: %.2E\n",iSolve.mean_time_per_iteration);
    printf("\n-------------------------------\n");

    atm.reset();
  }
	 }

fclose(fid0);
fclose(fid);
@ 
This is the place where we terminate the test elegantly.
<<cleanup and free memory>>=
HANDLE_ERROR( cudaFree( d__src) );
atm.cleanup();
lenslet_array.cleanup();
cog.cleanup();
aa.cleanup();
pa.cleanup();
aaCov.cleanup();
paCov.cleanup();
iSolve.cleanup();
S.cleanup();
HANDLE_ERROR( cudaFree( d__x ) );
HANDLE_ERROR( cudaFree( d__idx ) );
HANDLE_ERROR( cudaFree( d__ce ) );
HANDLE_ERROR( cudaFree( d__phase_est ) );
HANDLE_ERROR( cudaFree( d__phase_screen_low_res ) );
HANDLE_ERROR( cudaFree( d__mask ) );
free(phase_screen_low_res);
free(idx);
free(phase_screen_est);
free(x);
@ 
<<PA input>>=
__global__ void set_pa_input(float *pa_c, float *aa_c, int *idx, int N) 
{
  int i, j, k;
  i = blockIdx.x * blockDim.x + threadIdx.x;
  j = blockIdx.y * blockDim.y + threadIdx.y;
  if ( (i<N) && (j<N) ) {
    k = i*N + j;
    pa_c[idx[k]] = aa_c[k];
  }
}
@ 
This is a chunk of code for the [[statistics]] test routine:
<<atmosphere propagation and wavefront sensing>>=
/* atm.get_phase_screen(delta,PS_N_PX,delta,PS_N_PX,d__src,0); */
/* wf_rms = phase2nm*S.std(d__src->phase, _N_PIXEL_); */
/* printf("\n WF RMS: %7.2fnm\n",wf_rms); */

atm.get_phase_screen(d__phase_screen_low_res,delta_e,NP,delta_e,NP,d__src,0);

HANDLE_ERROR( cudaMemcpy( phase_screen_low_res, d__phase_screen_low_res,
			  sizeof(float)*PS_E_N_PX,
			  cudaMemcpyDeviceToHost ) );
fwrite(phase_screen_low_res,sizeof(float),PS_E_N_PX,fid0);

<<wavefront sensing (geometric and masked)>>
@ 
The wavefront sensing  model is using either a Fourier optics or geometric optics model.
<<wavefront sensing>>=
<<wavefront sensing (geometric)>>
<<save centroids to file>>
<<wavefront sensing (masked)>>=
<<wavefront sensing (geometric and masked)>>
<<save centroids to file>>
@ 
<<wavefront sensing (Fourier)>>=
lenslet_array.propagate(d__src);
cxy0 = (_N_PX_PUPIL_ - 1)/2.0;
cog.get_data(lenslet_array.d__frame, cxy0, cxy0, slopes2Angle);
@ 
<<wavefront sensing (geometric)>>=
atm.get_phase_screen_gradient(cog.d__cx,cog.d__cy,N_SIDE_LENSLET,d,d__src,0);
<<wavefront sensing (geometric and masked)>>=
atm.get_phase_screen_gradient(cog.d__cx,cog.d__cy,N_SIDE_LENSLET,d__mask,d,d__src,0);
@ 
<<save centroids to file>>=
HANDLE_ERROR( cudaMalloc((void**)&d__b, sizeof(float)*N ) );
HANDLE_ERROR( cudaMemcpy( d__b              , cog.d__c,  
			  N*sizeof(float), cudaMemcpyDeviceToDevice) );
b = (float *)malloc(sizeof(float)*N);
HANDLE_ERROR( cudaMemcpy( b, d__b,
			  sizeof(float)*N,
			  cudaMemcpyDeviceToHost ) );
fid = fopen("centroids.bin","wb");
fwrite(b,sizeof(float),N,fid);
fclose(fid);
@ 
The index of the WFS slopes in the augmented input vector of the MVM operation are computed next:
<<linear solver (prep.)>>=
idx = (int *)malloc(sizeof(int)*_N_LENSLET_);
k = -1;
//printf("\n k   idx\n");
for (i=1;i<2*N_SIDE_LENSLET;i+=2) {
  for (j=1;j<2*N_SIDE_LENSLET;j+=2) {
    idx[++k] = i*(2*N_SIDE_LENSLET + 1) + j;
    //printf("(%2d) %2d\n",k,idx[k]);
  }
 }
HANDLE_ERROR( cudaMalloc( (void**)&d__idx, sizeof(int)*_N_LENSLET_ ) );
HANDLE_ERROR( cudaMemcpy( d__idx, idx,
			  sizeof(int)*_N_LENSLET_,
			  cudaMemcpyHostToDevice ) );

HANDLE_ERROR( cudaMalloc((void**)&d__x, sizeof(float)*N ) );
x = (float*)malloc(sizeof(float)*N);
@ 
CG iterative solver call:
<<linear solver (conjugate gradient)>>=
iSolve.cg(d__x, &aaCov, d__b, 5, d__x);
@ 
MINRES iterative solver call:
<<linear solver (MINRES)>>=
iSolve.minres(d__x, &aaCov, d__b, 5, d__x);
@ 
Allocation of the input vector for the MVM:
<<MVM (prep.)>>=
HANDLE_ERROR( cudaMalloc((void**)&d__ce, sizeof(float)*PS_E_N_PX*2 ) );
HANDLE_ERROR( cudaMemset(d__ce, 0, sizeof(float)*PS_E_N_PX*2 ) );
dim3 blockDim(16,16);
dim3 gridDim(N_SIDE_LENSLET/16+1,N_SIDE_LENSLET/16+1);
@ 
Writing the input of the iterative solver into the output of the MVM:
<<MVM>>=
set_pa_input LLL gridDim,blockDim RRR (d__ce, d__x, d__idx, N_SIDE_LENSLET);
set_pa_input LLL gridDim,blockDim RRR (d__ce + PS_E_N_PX, d__x + _N_LENSLET_, d__idx, N_SIDE_LENSLET);
paCov.MVM(d__phase_est,d__ce);
@ 
Saving the MVM wavefront estimate to a file:
<<MVM (wrap.)>>=
phase_screen_est     = (float*)malloc(sizeof(float)*PS_E_N_PX);
HANDLE_ERROR( cudaMemcpy( phase_screen_est, d__phase_est,
			  sizeof(float)*PS_E_N_PX,
			  cudaMemcpyDeviceToHost ) );
fid = fopen(filename,"wb");
//fid = fopen("phaseScreenEst.bin","wb");
fwrite(phase_screen_est,sizeof(float),PS_E_N_PX,fid);
fclose(fid);
@ 
